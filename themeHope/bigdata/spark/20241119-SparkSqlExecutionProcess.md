---
# è¿™æ˜¯æ–‡ç« çš„æ ‡é¢˜
title: Sparksqlå…¨æµç¨‹åŸç†
# ä½ å¯ä»¥è‡ªå®šä¹‰å°é¢å›¾ç‰‡
#cover: /assets/images/cover1.jpg
# è¿™æ˜¯é¡µé¢çš„å›¾æ ‡
icon: file
# è¿™æ˜¯ä¾§è¾¹æ çš„é¡ºåº
order: 5
# è®¾ç½®ä½œè€…
author: bugcode
# è®¾ç½®å†™ä½œæ—¶é—´
date: 2024-11-19
# ä¸€ä¸ªé¡µé¢å¯ä»¥æœ‰å¤šä¸ªåˆ†ç±»
category:
  - SPARK
# ä¸€ä¸ªé¡µé¢å¯ä»¥æœ‰å¤šä¸ªæ ‡ç­¾
tag:
  - spark
  - å¤§æ•°æ®
# æ­¤é¡µé¢ä¼šåœ¨æ–‡ç« åˆ—è¡¨ç½®é¡¶
sticky: false
# æ­¤é¡µé¢ä¼šå‡ºç°åœ¨æ˜Ÿæ ‡æ–‡ç« ä¸­
star: true
# ä½ å¯ä»¥è‡ªå®šä¹‰é¡µè„š
footer: åˆ†å¸ƒå¼
# ä½ å¯ä»¥è‡ªå®šä¹‰ç‰ˆæƒä¿¡æ¯
copyright: bugcode
---


# Sparksqlå…¨æµç¨‹åŸç†

ç°åœ¨çš„å¤§æ•°æ®å¼€å‘å¾ˆå¤šå…¬å¸ä¸ºäº†æ•ˆç‡è€ƒè™‘ï¼Œå¤§å¤šæ•°ä½¿ç”¨sqlï¼Œä¸ç®¡æ˜¯hive sql è¿˜æ˜¯spark sqlï¼Œå…¶æ‰§è¡Œæµç¨‹å¤§å¤šå¾ˆç±»ä¼¼ï¼Œåœ¨æˆ‘çš„å·¥ä½œä¸­ï¼Œæ›´å¤šçš„æ˜¯ä½¿ç”¨spark sqlæ¥è¿›è¡Œä½œä¸šçš„å¼€å‘ï¼Œæ‰€ä»¥ç¬¬ä¸€ç¯‡ï¼Œæˆ‘ä»¬å…ˆæ¥ä»æ•´ä½“ä¸Šåˆ†æä¸€ä¸‹spark sqlçš„æ‰§è¡Œæµç¨‹ã€‚

# 1ã€æ‰§è¡Œæµç¨‹æ¦‚è¿°

sqlåœ¨è½¬æ¢ä¸ºRDDæ‰§è¡Œä¸­ä¼šç»è¿‡å¦‚ä¸‹å‡ ä¸ªé˜¶æ®µï¼š

![Untitled](https://vscodepic.oss-cn-beijing.aliyuncs.com/blog/Untitled.png)

**è¯æ³•åˆ†æ**ï¼šå°†è¾“å…¥çš„SQLè¯­å¥æ‹†è§£ä¸ºå•è¯åºåˆ—ï¼Œå¹¶è¯†åˆ«å‡ºå…³é”®å­—ã€æ ‡è¯†ã€å¸¸é‡ç­‰ä¿¡æ¯ï¼›

**è¯­æ³•åˆ†æ**ï¼šæ£€æŸ¥è¯æ³•åˆ†æè§£æå‡ºæ¥çš„å•è¯åºåˆ—æ˜¯å¦æ»¡è¶³SQLè¯­æ³•è§„åˆ™ï¼›

**é€»è¾‘è®¡åˆ’**ï¼šè¯­æ³•åˆ†æå®Œçš„ç»“æœä¼šç”ŸæˆåŸå§‹çš„é€»è¾‘è®¡åˆ’ä¿¡æ¯ï¼Œåœ¨è¿™ä¸ªé˜¶æ®µä¼šè¿›è¡Œåˆ†æå’Œä¼˜åŒ–å¤„ç†ï¼Œå…·ä½“ä¼šåˆ†ä¸ºå¦‚ä¸‹å‡ ä¸ªé˜¶æ®µï¼š

![Untitled](https://vscodepic.oss-cn-beijing.aliyuncs.com/blog/Untitled%201.png)

**ç‰©ç†è®¡åˆ’**ï¼šç‰©ç†è®¡åˆ’ä¼šå°†ä¸Šä¸€æ­¥çš„é€»è¾‘è®¡åˆ’è¿›ä¸€æ­¥è½¬æ¢ï¼Œç”Ÿæˆå¯ä»¥æ‰§è¡Œçš„è®¡åˆ’ä¿¡æ¯

![Untitled](https://vscodepic.oss-cn-beijing.aliyuncs.com/blog/Untitled%202.png)

å¦‚æœç”¨ä¸€å¼ å›¾å±•ç¤ºï¼Œé‚£æˆ‘ä»¬å¼€å‘å®Œä¸€ä¸ªsqlä»»åŠ¡ï¼Œå®ƒçš„æ€»çš„æ‰§è¡Œæµç¨‹æ˜¯è¿™æ ·çš„ï¼š

![Untitled](https://vscodepic.oss-cn-beijing.aliyuncs.com/blog/Untitled.jpeg)

å…¶ä¸­æ¯éš”ä¸ªé˜¶æ®µåˆåšäº†ä¸åŒçš„å·¥ä½œï¼š

## 1.1ã€Unresolved-æœªè§£æé˜¶æ®µ

æ­¤é˜¶æ®µä¸»è¦åšäº†ä¸¤ä»¶äº‹ï¼š

1ã€å°†sqlå­—ç¬¦ä¸²é€šè¿‡antrl4è½¬åŒ–æˆASTé€»è¾‘è¯­æ³•æ ‘

2ã€å°†ASTè¯­æ³•æ ‘ç»è¿‡sparkè‡ªå®šä¹‰è®¿é—®è€…æ¨¡å¼è½¬åŒ–æˆlogicalPlanã€å¯ä»¥ç†è§£ä¸ºç²¾ç®€ç‰ˆè¯­æ³•æ ‘ã€‘

è¯¥é˜¶æ®µè¯­æ³•æ ‘ä¸­ä»…ä»…æ˜¯æ•°æ®ç»“æ„ï¼Œä¸åŒ…å«ä»»ä½•æ•°æ®ä¿¡æ¯

## 1.2ã€Analyzed-åˆ†æé˜¶æ®µ

æ­¤é˜¶æ®µä¼šå°†ä¸Šä¸€é˜¶æ®µè·å¾—çš„é€»è¾‘è¯­æ³•æ ‘å’Œå…ƒæ•°æ®è¿›è¡Œç»‘å®šï¼Œæ„å»ºå‡ºæ›´åŒ…å«å…ƒæ•°æ®çš„é€»è¾‘è¯­æ³•æ ‘

å¦‚æœæ˜¯hivesqlï¼Œåˆ™æ­¤é˜¶æ®µä¼šå’Œhive-catalogè¿›è¡Œäº¤äº’

## 1.3ã€Optimized-ä¼˜åŒ–é˜¶æ®µ

æ­¤é˜¶æ®µæ˜¯å¯¹åˆ†æé˜¶æ®µçš„é€»è¾‘è®¡åˆ’åšè¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå°†åº”ç”¨å„ç§ä¼˜åŒ–è§„åˆ™å¯¹ä¸€äº›ä½æ•ˆçš„é€»è¾‘è®¡åˆ’è¿›è¡Œè½¬æ¢

ä¾‹å¦‚å°†åŸæœ¬ç”¨æˆ·ä¸åˆç†çš„sqlè¿›è¡Œä¼˜åŒ–ï¼Œå¦‚è°“è¯ä¸‹æ¨ï¼Œåˆ—è£å‰ªï¼Œå­æŸ¥è¯¢å…±ç”¨ç­‰ã€‚

## 1.4ã€PhysicalPlan-ç‰©ç†è®¡åˆ’é˜¶æ®µ

ç‰©ç†è®¡åˆ’é˜¶æ®µå°†ä¸Šä¸€æ­¥é€»è¾‘è®¡åˆ’é˜¶æ®µç”Ÿæˆçš„é€»è¾‘ç®—å­æ ‘è¿›è¡Œè¿›ä¸€æ­¥è½¬æ¢ï¼Œç”Ÿæˆç‰©ç†ç®—å­æ ‘ï¼Œç‰©ç†ç®—å­æ ‘çš„èŠ‚ç‚¹ä¼šç›´æ¥ç”Ÿæˆ RDD æˆ–å¯¹ RDD è¿›è¡Œ transformation æ“ä½œï¼›

## 1.5ã€Cost-Model-ç­–ç•¥é€‰å–é˜¶æ®µ

Cost Model å¯¹åº”çš„å°±æ˜¯åŸºäºä»£ä»·çš„ä¼˜åŒ–ï¼ˆCost-based Optimizationsï¼ŒCBOï¼Œè¯¦è§ SPARK-16026 ï¼‰ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯è®¡ç®—æ¯ä¸ªç‰©ç†è®¡åˆ’çš„ä»£ä»·ï¼Œç„¶åå¾—åˆ°æœ€ä¼˜çš„ç‰©ç†è®¡åˆ’ã€‚ä½†æ˜¯æˆªæ­¢åˆ°spark3.0ï¼Œè¿™ä¸€éƒ¨åˆ†å¹¶æ²¡æœ‰å®ç°

## 1,6ã€Prepared-é¢„æäº¤é˜¶æ®µ

æ­¤èŠ‚ç‚¹æ˜¯æäº¤å‰çš„ä¼˜åŒ–ï¼Œå†…éƒ¨æ˜¯åŒçš„è§„åˆ™ä½œç”¨åœ¨sparkPlanç‰©ç†æ ‘ï¼Œä»è€Œè·å¾—ä¼˜åŒ–åçš„å¯æ‰§è¡Œçš„ç‰©ç†æ ‘ã€‚

å…¶ä¸­æœ‰ä¸ªé‡å¤´æˆï¼Œå°±æ˜¯spark3.0ä¸­æ–°å¢åŠŸèƒ½AQEã€è‡ªé€‚åº”æ‰§è¡Œã€‘ï¼ŒSPARK-9850 åœ¨ Spark ä¸­æå‡ºäº†è‡ªé€‚åº”æ‰§è¡Œçš„åŸºæœ¬æ€æƒ³ï¼Œå…³äºåŠŸèƒ½å®ç°ä¸åœ¨è¿™é‡Œè¿‡å¤šé™ˆè¿°ï¼Œå¯æŸ¥çœ‹ç›¸å…³æ–‡çŒ®ï¼›ç”±æ­¤å¯ä»¥çœ‹å‡ºAQEåŠŸèƒ½ç›®å‰åªèƒ½é€šè¿‡sparksqlæ‰èƒ½ä½¿ç”¨

# 2ã€sparksqlè¯»å–pgæ•°æ®

æ¥ä¸‹æ¥æˆ‘ä»¬å°†ä»postgresqlæ•°æ®åº“ä¸­è¯»å–ä¸€å¼ è¡¨æ•°æ®,è¡¨ç»“æ„å¦‚ä¸‹ï¼š

```scala
+---+------+-----------------+-----+------+------+--------+--------+-------------------+
|cid| cname|         caddress|ccity|cstate|  czip|ccountry|ccontact|             cemail|
+---+------+-----------------+-----+------+------+--------+--------+-------------------+
|  1|  é©¬äº‘|       å¤§é©¬è·¯ä¸€å·| æ­å·|  æµ™æ±Ÿ|110120|    ä¸­å›½|    é˜¿äº‘|  mayun@alibaba.com|
|  2|åˆ˜å¼ºä¸œ|ä¸­å…³æ‘ä¸Šåœ°ä¸€è¡—2å·| åŒ—äº¬|  åŒ—äº¬|100000|    ä¸­å›½|å¥¶èŒ¶å¦¹å¦¹|liuqiangdong@jd.com|
|  3|é©¬åŒ–è…¾|           ä½ æ‡‚å¾—| ä¸œè|  å¹¿ä¸œ|300000|    ä¸­å›½|  é©¬åŒ–è…¾|   mahuateng@qq.com|
+---+------+-----------------+-----+------+------+--------+--------+-------------------+

```

ä»sparksqlè¯»å–æ•°æ®åº“ä¸­çš„æ•°æ®å¼€å§‹ï¼š

```scala
val sql =
			"""
				|select * from cust where cid <=2 and cname != 'åˆ˜å¼ºä¸œ'
				|""".stripMargin

		session.sql(sql).show()
		session.sql(sql).explain(true)
```

spark.sql æ¥è‡ª `SparkSession.scala` æ–‡ä»¶ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å…·ä½“çš„æºç ï¼š

```scala
/**
   * ä½¿ç”¨Sparkæ‰§è¡ŒSQLæŸ¥è¯¢ï¼Œå¹¶å°†ç»“æœä½œä¸ºâ€œDataFrameâ€è¿”å›ã€‚
   */
  def sql(sqlText: String): DataFrame = {
//ä¸‹é¢è¿™ä¸ªæ–¹æ³•æ€»å…±åšäº†ä¸¤æ­¥
	// 1  parsing é˜¶æ®µ
	// 2 parsing é˜¶æ®µç”Ÿæˆçš„é€»è¾‘è®¡åˆ’ç»è¿‡å¤„ç†ç”Ÿæˆ DataFrame è¿”å›
    Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))
  }
```

åœ¨.ofRows()æ–¹æ³•ä¸­ï¼Œæœ‰ä¸€ä¸ªå‚æ•°sessionStateï¼Œè¿™ä¸ªå¯¹è±¡é‡Œé¢å­˜å‚¨äº†sqlåœ¨æ‰§è¡ŒæœŸé—´çš„æ‰€æœ‰æ•°æ®çŠ¶æ€ï¼Œæ‰€ä»¥æ¥ä¸‹æ¥è¯¦ç»†ä»‹ç»è¿™ä¸ªå¯¹è±¡ã€‚

# 3ã€**SessionState**

## 3.1ã€ä»‹ç»

Apache Spark 2.0å¼•å…¥äº†SparkSessionï¼Œå…¶ç›®çš„æ˜¯ä¸ºç”¨æˆ·æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ‡å…¥ç‚¹æ¥ä½¿ç”¨Sparkçš„å„é¡¹åŠŸèƒ½ï¼Œä¸å†éœ€è¦æ˜¾å¼åœ°åˆ›å»ºSparkConf, SparkContext ä»¥åŠ SQLContextï¼Œå› ä¸ºè¿™äº›å¯¹è±¡å·²ç»å°è£…åœ¨SparkSessionä¸­ã€‚æ­¤å¤–SparkSessionå…è®¸ç”¨æˆ·é€šè¿‡å®ƒè°ƒç”¨DataFrameå’ŒDatasetç›¸å…³APIæ¥ç¼–å†™Sparkç¨‹åºã€‚

é‚£ä¹ˆåœ¨sparkSqlæ¨¡å—ä¸­ï¼Œsqlå„ä¸ªé˜¶æ®µçš„è§£æçš„æ ¸å¿ƒç±»åˆ™æ˜¯SessionStateï¼Œåœ¨åç»­çš„æ–‡ç« ä¸­ä¼šå¤šæ¬¡ä½¿ç”¨åˆ°SessionStateçš„å˜é‡ï¼Œæ•…æœ¬èŠ‚å°†ä»‹ç»SessionStateæ˜¯å¦‚ä½•æ„å»ºçš„

## 3.2ã€sparksessionçš„åˆ›å»º

```scala
// TODO åˆ›å»ºSparkSQLçš„è¿è¡Œç¯å¢ƒ
val sparkConf = new SparkConf().setMaster("local[2]").setAppName("sparkSQL")
val spark = SparkSession.builder().config(sparkConf)
  .getOrCreate()

// hiveå¦‚ä¸‹ï¼š
val spark = SparkSession.builder().config(sparkConf)
  .enableHiveSupport()
  .getOrCreate()
```

å¯ä»¥çœ‹åˆ°ï¼Œsparksessionçš„åˆ›å»ºä½¿ç”¨çš„åˆ›å»ºè€…è®¾è®¡æ¨¡å¼ï¼Œè¿™æ ·sparkå°±å¯ä»¥ä½¿ç”¨.config()çš„æ–¹æ³•é…ç½®å±æ€§ç„¶åæ„å»ºå¯¹è±¡ã€‚

### 3.2.1ã€`getOrCreate()`

<aside>
ğŸ’¡ æ–¹æ³•è¯´æ˜


```xml
Gets an existing [[SparkSession]] or, if there is no existing one, creates a new one based on the options set in this builder.
This method first checks whether there is a valid thread-local SparkSession,
and if yes, return that one. It then checks whether there is a valid global
default SparkSession, and if yes, return that one. If no valid global default SparkSession exists, the method creates a new SparkSession and assigns the
newly created SparkSession as the global default.
In case an existing SparkSession is returned, the non-static config options specified in this builder will be applied to the existing SparkSession.

//sparkSessionç±»

class SparkSession private(
    @transient val sparkContext: SparkContext,
    @transient private val existingSharedState: Option[SharedState],
    @transient private val parentSessionState: Option[SessionState],
    @transient private[sql] val extensions: SparkSessionExtensions)
  extends Serializable with Closeable with Logging {}
```

</aside>

```scala
def getOrCreate(): SparkSession = synchronized {
// SparkSessionåªèƒ½åœ¨Driverç«¯åˆ›å»ºå’Œè®¿é—®
      assertOnDriver()
// é¦–å…ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨æœ‰æ•ˆçš„çº¿ç¨‹æœ¬åœ°SparkSessionï¼Œå¦‚æœsessionä¸ä¸ºç©º,
//ä¸”sessionå¯¹åº”çš„sparkContextæœªåœæ­¢äº†,è¿”å›ç°æœ‰session
      // Get the session from current thread's active session.
      var session = activeThreadSession.get()
      if ((session ne null) && !session.sparkContext.isStopped) {
        applyModifiableSettings(session)
        return session
      }
// çº¿ç¨‹åŒæ­¥æ‰§è¡Œ
      // Global synchronization so we will only set the default session once.
      SparkSession.synchronized {
// å¦‚æœå½“å‰çº¿ç¨‹æ²¡æœ‰æ´»åŠ¨ä¼šè¯ï¼Œè¯·ä»å…¨å±€ä¼šè¯è·å–å®ƒ
        // If the current thread does not have an active session, get it from the global session.
        session = defaultSession.get()
        if ((session ne null) && !session.sparkContext.isStopped) {
          applyModifiableSettings(session)
          return session
        }
// æ²¡æœ‰æ´»åŠ¨æˆ–å…¨å±€é»˜è®¤ä¼šè¯ã€‚åˆ›å»ºä¸€ä¸ªæ–°çš„sparkContext
        // No active nor global default session. Create a new one.
        val sparkContext = userSuppliedContext.getOrElse {
          val sparkConf = new SparkConf()
          options.foreach { case (k, v) => sparkConf.set(k, v) }

          // set a random app name if not given.
          if (!sparkConf.contains("spark.app.name")) {
            sparkConf.setAppName(java.util.UUID.randomUUID().toString)
          }

          SparkContext.getOrCreate(sparkConf)
          // Do not update `SparkConf` for existing `SparkContext`, as it's shared by all sessions.
        }

        // Initialize extensions if the user has defined a configurator class.
        val extensionConfOption = sparkContext.conf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS)
        if (extensionConfOption.isDefined) {
          val extensionConfClassName = extensionConfOption.get
          try {
            val extensionConfClass = Utils.classForName(extensionConfClassName)
            val extensionConf = extensionConfClass.newInstance()
              .asInstanceOf[SparkSessionExtensions => Unit]
            extensionConf(extensions)
          } catch {
            // Ignore the error if we cannot find the class or when the class has the wrong type.
            case e @ (_: ClassCastException |
                      _: ClassNotFoundException |
                      _: NoClassDefFoundError) =>
              logWarning(s"Cannot use $extensionConfClassName to configure session extensions.", e)
          }
        }
// é‡ç‚¹æ­¤å¤„æ„å»ºSparkSessionï¼›extensionså…¥å‚ä¸ºè‡ªå®šä¹‰æ‰©å±•ç±»ï¼Œåé¢ä¼šæœ‰ä¸€èŠ‚å•ç‹¬ä»‹ç»ã€‚
        session = new SparkSession(sparkContext, None, None, extensions)
        options.foreach { case (k, v) => session.initialSessionOptions.put(k, v) }
        setDefaultSession(session)
        setActiveSession(session)

        // Register a successfully instantiated context to the singleton. This should be at the
        // end of the class definition so that the singleton is updated only if there is no
        // exception in the construction of the instance.
        sparkContext.addSparkListener(new SparkListener {
          override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {
            defaultSession.set(null)
          }
        })
      }

      return session
    }
```

## 3.3ã€`SessionStateç±»`

**åœ¨SparkSessionç±»ä¸­æœ‰ä¸€ä¸ªæ ¸å¿ƒå±æ€§ï¼šSessionStateï¼Œè¯¥å±æ€§å­˜å‚¨ç€sparksqlå„ä¸ªé˜¶æ®µçš„æ‰§è¡Œè¿‡ç¨‹ï¼Œååˆ†é‡è¦ï¼š**

```scala
/**
   * State isolated across sessions, including SQL configurations, temporary tables, registered
   * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]].
   * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent.
   *
   * This is internal to Spark and there is no guarantee on interface stability.
   *
   * @since 2.2.0
   */
  @InterfaceStability.Unstable
  @transient
  lazy val sessionState: SessionState = {
    parentSessionState
      .map(_.clone(this))
      .getOrElse {
        val state = SparkSession.instantiateSessionState(
          SparkSession.sessionStateClassName(sparkContext.conf),
          self)
        initialSessionOptions.foreach { case (k, v) => state.conf.setConfString(k, v) }
        state
      }
  }
```

### 3.3.1ã€å±æ€§

ä¸‹é¢æ˜¯sessionStateç±»çš„å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹çœ‹éƒ½ä¿å­˜äº†å“ªäº›çŠ¶æ€ä¿¡æ¯ã€‚

```scala
private[sql] class SessionState(
    sharedState: SharedState,
    val conf: SQLConf,
    val experimentalMethods: ExperimentalMethods,
    val functionRegistry: FunctionRegistry,
    val udfRegistration: UDFRegistration,
    catalogBuilder: () => SessionCatalog,
    val sqlParser: ParserInterface,
    analyzerBuilder: () => Analyzer,
    optimizerBuilder: () => Optimizer,
    val planner: SparkPlanner,
    val streamingQueryManager: StreamingQueryManager,
    val listenerManager: ExecutionListenerManager,
    resourceLoaderBuilder: () => SessionResourceLoader,
    createQueryExecution: LogicalPlan => QueryExecution,
    createClone: (SparkSession, SessionState) => SessionState) {}
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æºç ä¸­å¯¹è¿™ä¸ªç±»çš„è¯´æ˜ï¼š

```scala
A class that holds all session-specific state in a given [[SparkSession]].
åŒ…å«äº†æ‰€æœ‰sparksessionä¼šè¯æœŸé—´çš„çŠ¶æ€ä¿¡æ¯
```

`sharedState`:ä¼šè¯æœŸé—´çš„å…±äº«çŠ¶æ€ï¼Œä¾‹å¦‚å…¨å±€è§†å›¾ç®¡ç†å™¨ï¼Œå¤–éƒ¨ç›®å½•ç­‰ã€‚

`conf`:ç‰¹å¾çš„k-vç±»å‹çš„sqlé…ç½®æ–‡ä»¶

`experimentalMethods`ï¼šç”¨äºæ·»åŠ è‡ªå®šä¹‰è®¡åˆ’ç­–ç•¥å’Œä¼˜åŒ–å™¨çš„æ¥å£ã€‚

`functionRegistry`ï¼šç”¨äºç®¡ç†ç”¨æˆ·æ³¨å†Œçš„åŠŸèƒ½çš„å†…éƒ¨ç›®å½•ã€‚

`udfRegistration`ï¼šå‘ç”¨æˆ·å…¬å¼€çš„ç”¨äºæ³¨å†Œç”¨æˆ·å®šä¹‰å‡½æ•°çš„æ¥å£ã€‚

`catalogBuilder`ï¼šåˆ›å»ºç”¨äºç®¡ç†è¡¨å’Œæ•°æ®åº“çŠ¶æ€çš„å†…éƒ¨ç›®å½•çš„å‡½æ•°ã€‚

`sqlParser`ï¼šä»SQLæ–‡æœ¬ä¸­æå–è¡¨è¾¾å¼ã€è®¡åˆ’ã€è¡¨æ ‡è¯†ç¬¦ç­‰çš„è§£æå™¨ã€‚

`analyzerBuilder`ï¼šç”¨äºåˆ›å»ºé€»è¾‘æŸ¥è¯¢è®¡åˆ’åˆ†æå™¨çš„å‡½æ•°ï¼Œç”¨äºè§£ææœªè§£æçš„å±æ€§å’Œå…³ç³»ã€‚

`optimizerBuilder`ï¼šç”¨äºåˆ›å»ºé€»è¾‘æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–å™¨çš„å‡½æ•°ã€‚

`planner`ï¼šå°†ä¼˜åŒ–çš„é€»è¾‘è®¡åˆ’è½¬æ¢ä¸ºç‰©ç†è®¡åˆ’çš„è®¡åˆ’å™¨ã€‚

`streamingQueryManager`ï¼šç”¨äºå¯åŠ¨å’Œåœæ­¢æµå¼æŸ¥è¯¢çš„æ¥å£ã€‚

`listenerManager`ï¼šç”¨äºæ³¨å†Œè‡ªå®šä¹‰[[QueryExecutionListener]]çš„æ¥å£ã€‚

`resourceLoaderBuilder`ï¼šåˆ›å»ºä¼šè¯å…±äº«èµ„æºåŠ è½½å™¨ä»¥åŠ è½½JARã€æ–‡ä»¶ç­‰çš„å‡½æ•°ã€‚

`createQueryExecution`ï¼šç”¨äºåˆ›å»ºQueryExecutionå¯¹è±¡çš„å‡½æ•°ã€‚

`createClone`ï¼šç”¨äºåˆ›å»ºä¼šè¯çŠ¶æ€çš„å…‹éš†çš„å‡½æ•°ã€‚

**SessionStateä¸­åŒ…å«äº†å¾ˆå¤šçš„å±æ€§ï¼Œè¿™äº›å±æ€§åœ¨æ‰§è¡ŒsparksqlæœŸé—´å­˜å‚¨å„ä¸ªé˜¶æ®µçš„æ•°æ®ä¿¡æ¯ï¼Œå…¶ä¸­æ¯”è¾ƒé‡è¦çš„æœ‰ä¸‹é¢å‡ ä¸ªï¼š**

**SessionCatalogï¼š**

æŒ‰ç…§ SQL æ ‡å‡†ï¼ŒCatalog æ˜¯ä¸€ä¸ªå®½æ³›æ¦‚å¿µï¼Œé€šå¸¸å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªå®¹å™¨æˆ–æ•°æ®åº“å¯¹è±¡å‘½åç©ºé—´ä¸­çš„ä¸€ä¸ªå±‚æ¬¡ï¼Œä¸»è¦ç”¨æ¥è§£å†³å‘½åå†²çªç­‰é—®é¢˜ã€‚

åœ¨ Spark SQL ç³»ç»Ÿä¸­ï¼ŒCatalog ä¸»è¦äºå„ç§å‡½æ•°å…ƒä¿¡æ¯ï¼ˆæ•°æ®åº“ã€æ•°æ®è¡¨ã€æ•°æ®è§†å›¾ã€æ•°æ®åˆ†åŒºä¸å‡½æ•°ç­‰ï¼‰çš„ç»Ÿä¸€ç®¡ç†ã€‚

å…·ä½“æ¥è®²ï¼ŒSpark SQL ä¸­Catalog ä½“ç³»å°±æ˜¯ä»¥ SessionCatalog ä¸ºä¸»ï¼Œé€šè¿‡ SparkSession æ¥æä¾›ç»™å¤–ç•Œè°ƒç”¨ã€‚ä¸€èˆ¬ï¼Œä¸€ä¸ª SparkSession å¯¹åº”ä¸€ä¸ªSessionCatalogã€‚

æœ¬è´¨ä¸Šï¼ŒSessionCatalog èµ·åˆ°äº†ä¸€ä¸ªä»£ç†çš„ä½œç”¨ï¼Œå¯¹åº•å±‚çš„å…ƒæ•°æ®ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼ŒHive Metastoreï¼‰ã€ä¸´æ—¶è¡¨ä¿¡æ¯ã€è§†å›¾ä¿¡æ¯å’Œå‡½æ•°åšäº†å°è£…ã€‚

**ParserInterface**

ç¼–è¯‘å™¨é€šç”¨æ¥å£ï¼Œç”¨æ¥å°† SQL è¯­å¥è½¬æ¢æˆ **AST**ï¼ˆæŠ½è±¡è¯­æ³•æ ‘ï¼‰ï¼Œä¹Ÿå°±æ˜¯ Unresolved Logical Planï¼Œè¿™ä¸ªæ¥å£ä¸­åŒ…å«å¯¹ SQL è¯­å¥ã€Expression è¡¨è¾¾å¼å’Œ TableIdentifier æ•°æ®è¡¨æ ‡è¯†ç¬¦çš„è§£ææ–¹æ³•ï¼Œä¸‹é¢æ˜¯æ¥å£çš„å®ç°å’Œç»§æ‰¿å…³ç³»ï¼š

![Untitled](https://vscodepic.oss-cn-beijing.aliyuncs.com/blog/Untitled%203.png)

ParserInterface æœ‰ä¸¤ä¸ªä¸»è¦å®ç°ç±»ï¼š

1. SparkSqlParser ç”¨äºå¤–éƒ¨è°ƒç”¨ï¼Œæˆ‘ä»¬å¹³å¸¸å†™çš„ SQL éƒ½æ˜¯å®ƒåœ¨è§£æã€‚
2. CatalystSqlParser ç”¨äºå†…éƒ¨ Catalyst å¼•æ“ä½¿ç”¨çš„è§£æå™¨ã€‚

å½“æˆ‘ä»¬ä»è¡¨ä¸­è¯»å–æ•°æ®ï¼Œæœªè§£æçš„é€»è¾‘æ‰§è¡Œè®¡åˆ’æ˜¯ä¸å¸¦ä»»ä½•ç±»å‹ä¿¡æ¯çš„æ•°æ®ã€‚

```xml
== Parsed Logical Plan ==
'Project [*]
+- 'Filter (('cid <= 2) && NOT ('cname = åˆ˜å¼ºä¸œ))
   +- 'UnresolvedRelation `cust`
```

Analyzer

æä¾›é€»è¾‘æŸ¥è¯¢è®¡åˆ’çš„åˆ†æå™¨ï¼Œè¿™ä¸ªåˆ†æå™¨ä¼šä½¿ç”¨ SessionCatalog ä¸­çš„ä¿¡æ¯å°† UnsolvedAttributes å’Œ UnsolvedRelationships è½¬æ¢ä¸ºæœ‰ç±»å‹çš„å¯¹è±¡ã€‚

å…¶ä¸­ï¼ŒUnsolvedAttributes ä¿å­˜å°šæœªè§£æçš„å±æ€§çš„åç§°ï¼ŒUnsolvedRelationships ä¿å­˜å°šæœªåœ¨SessionCatalogä¸­æŸ¥æ‰¾çš„å…³ç³»çš„åç§°ã€‚

ç®€å•æ¥è®²ï¼Œå°±æ˜¯å°† Unresolved Logical Plan è½¬åŒ–æˆ Analyzed Logical Planã€‚

è¿™ä¸ªè¿‡ç¨‹ä¸»è¦ä¼šç»“åˆ DataFrame çš„ Schema ä¿¡æ¯ï¼ˆæ¥è‡ª SessionCatalogï¼‰ï¼Œæ£€æŸ¥ä¸‹é¢ 3 ç‚¹ï¼š

- è¡¨åç§°
- å­—æ®µåç§°
- å­—æ®µç±»å‹

```xml
== Analyzed Logical Plan ==
cid: int, cname: string, caddress: string, ccity: string, cstate: string, czip: string, ccountry: string, ccontact: string, cemail: string
Project [cid#0, cname#1, caddress#2, ccity#3, cstate#4, czip#5, ccountry#6, ccontact#7, cemail#8]
+- Filter ((cid#0 <= 2) && NOT (cname#1 = åˆ˜å¼ºä¸œ))
   +- SubqueryAlias `cust`
      +- Relation[cid#0,cname#1,caddress#2,ccity#3,cstate#4,czip#5,ccountry#6,ccontact#7,cemail#8] JDBCRelation(public.customer) [numPartitions=1]
```

ä»ä¸Šé¢è§£æè¿‡çš„æ‰§è¡Œè®¡åˆ’å¯ä»¥çœ‹åˆ°ã€‚å·²ç»ç»™è¯»å‡ºæ¥çš„å„ä¸ªå­—æ®µç»‘å®šç±»å‹ï¼Œå¹¶ä¸”åœ¨è§£æè¿‡ç¨‹ä¸­ä½¿ç”¨çš„åˆ—åæ˜¯cid#0ï¼Œå­æŸ¥è¯¢çš„è¡¨åæ˜¯cust.

Optimizer

æ‰€æœ‰ä¼˜åŒ–å™¨éƒ½åº”è¯¥ç»§æ‰¿çš„æŠ½è±¡ç±»ï¼ŒåŒ…å«æ ‡å‡†è§„åˆ™æ‰¹æ¬¡ï¼ˆæ‰©å±•ä¼˜åŒ–å™¨å¯ä»¥è¦†ç›–å®ƒï¼‰ã€‚

å…¶å®ä¾‹ç±»æ˜¯ SparkOptimizer

Optimizer ä¼šåŸºäºå¯å‘å¼çš„è§„åˆ™ï¼Œå°† Analyzed Logical Plan è½¬åŒ–æˆ Optimized Logical Plan ã€‚

å…¶ä¸­ï¼Œå¯å‘å¼çš„è§„åˆ™ä¸»è¦æ¶‰åŠä»¥ä¸‹ 3 ä¸ªæ–¹é¢çš„ä¼˜åŒ–ï¼š

1. è°“è¯ä¸‹æ¨ï¼Œç®€å•æ¥è®²å°±æ˜¯æŠŠè¿‡æ»¤æ“ä½œå°½å¯èƒ½å¾€æ•°æ®æºæ–¹å‘ç§»åŠ¨ï¼Œè¿™æ ·å¯ä»¥å‡å°‘è®¡ç®—å­˜å‚¨çš„è´Ÿè½½ã€‚
2. åˆ—å‰ªè£ï¼Œç®€å•æ¥è®²å°±æ˜¯æ ¹æ®åˆ—å­˜å‚¨æ ¼å¼ footer ä¸­çš„å…ƒæ•°æ®ä¿¡æ¯æ¥ç»†ç²’åº¦æå–æƒ³è¦çš„æ•°æ®ï¼Œä»è€Œå‡å°‘ IO æ¶ˆè€—ã€‚è¿™é‡Œåªæœ‰åˆ—å­˜å‚¨æ ¼å¼æ¯”å¦‚ ORC æˆ–è€… Parquet æ‰èƒ½äº«å—è¿™ä¸ªä¼˜åŒ–ã€‚
3. å¸¸é‡æŠ˜å ï¼Œç®€å•æ¥è®²å°±æ˜¯æŠŠä¸€äº›å¯ä»¥ç›´æ¥è®¡ç®—å¾—åˆ°çš„ç»“æœæ›¿æ¢æ‰æœ¬æ¥çš„è¡¨è¾¾å¼ï¼Œæ¯”å¦‚ 1+1=2 è¿™æ ·çš„ã€‚

```xml
== Optimized Logical Plan ==
Filter (((isnotnull(cid#0) && isnotnull(cname#1)) && (cid#0 <= 2)) && NOT (cname#1 = åˆ˜å¼ºä¸œ))
+- Relation[cid#0,cname#1,caddress#2,ccity#3,cstate#4,czip#5,ccountry#6,ccontact#7,cemail#8] JDBCRelation(public.customer) [numPartitions=1]
```

SparkPlanner

åŸºäºæ—¢å®šçš„è§„åˆ™å°†é€»è¾‘è®¡åˆ’è½¬æ¢æˆå…·ä½“çš„ç‰©ç†è®¡åˆ’ï¼ˆä¸æ¶‰åŠæ‰§è¡Œï¼Œåªæ˜¯æå‰è§„åˆ’ï¼‰ï¼Œå³å°†Optimized Logical Plan è½¬åŒ–æˆ Physical Planã€‚

> ç®€å•æ¥è¯´ï¼Œé€»è¾‘è®¡åˆ’å°±æ˜¯â€œåº”è¯¥åšä»€ä¹ˆâ€ï¼Œç‰©ç†è®¡åˆ’å°±æ˜¯â€œå…·ä½“æ€ä¹ˆåšâ€ã€‚

ç‰©ç†è®¡åˆ’åŒ…å« 3 ä¸ªå­é˜¶æ®µï¼š

1. é¦–å…ˆï¼Œæ ¹æ®é€»è¾‘ç®—å­æ ‘ï¼Œç”Ÿæˆç‰©ç†ç®—å­æ ‘çš„åˆ—è¡¨
2. å…¶æ¬¡ï¼Œä»åˆ—è¡¨ä¸­æŒ‰ç…§ä¸€å®šçš„ç­–ç•¥é€‰å–æœ€ä¼˜çš„ç‰©ç†ç®—å­æ ‘
3. æœ€åï¼Œå¯¹é€‰å–çš„ç‰©ç†ç®—å­æ ‘è¿›è¡Œæäº¤å‰çš„å‡†å¤‡å·¥ä½œï¼Œä¾‹å¦‚ï¼Œç¡®ä¿åˆ†åŒºæ“ä½œæ­£ç¡®ã€ç‰©ç†ç®—å­æ ‘èŠ‚ç‚¹é‡ç”¨ã€æ‰§è¡Œä»£ç ç”Ÿæˆç­‰ï¼Œå¾—åˆ° Prepared SparkPlanã€‚

æœ€ç»ˆç”Ÿæˆçš„æ˜¯ä¸€ä¸ª RDD å¯¹è±¡ï¼Œå¹¶å°†å…¶æäº¤ç»™ Spark Core æ¥æ‰§è¡Œã€‚

```xml
== Physical Plan ==
*(1) Scan JDBCRelation(public.customer) [numPartitions=1] [cid#0,cname#1,caddress#2,ccity#3,cstate#4,czip#5,ccountry#6,ccontact#7,cemail#8] PushedFilters: [*IsNotNull(cid), *IsNotNull(cname), *LessThanOrEqual(cid,2), *Not(EqualTo(cname,åˆ˜å¼ºä¸œ))], ReadSchema: struct<cid:int,cname:string,caddress:string,ccity:string,cstate:string,czip:string,ccountry:strin...
```

åœ¨sessionStateé‡Œé¢çš„è€…å››ä¸ªæ–¹æ³•ï¼Œå°±æ˜¯è¿™ä¸ªsqlçš„æ‰§è¡Œæµç¨‹ã€‚

```scala
**// The following fields are lazy to avoid creating the Hive client when creating SessionState.
  lazy val catalog: SessionCatalog = catalogBuilder()

  lazy val analyzer: Analyzer = analyzerBuilder()

  lazy val optimizer: Optimizer = optimizerBuilder()

  lazy val resourceLoader: SessionResourceLoader = resourceLoaderBuilder()**
```

å›åˆ°SparkSessionæ„å»ºSessionStateè¿‡ç¨‹,å¯ä»¥çœ‹åˆ°å…ˆæ˜¯é€šè¿‡configçš„CATALOG_IMPLEMENTATIONå±æ€§åˆ†è¾¨æ„å»ºå‡ºä¸¤ç§SessionStateï¼š

1ã€HiveSessionStateBuilder

2ã€SessionStateBuilder

```scala
//åˆ›å»ºsessionstate
@InterfaceStability.Unstable
  @transient
  lazy val sessionState: SessionState = {
    parentSessionState
      .map(_.clone(this))
      .getOrElse {
// è°ƒç”¨instantiateSessionStateå‡½æ•°é€šè¿‡ä¼ å…¥çš„å…¨ç±»ååˆ›å»ºsession
        val state = SparkSession.instantiateSessionState(
          SparkSession.sessionStateClassName(sparkContext.conf),
          self)
        initialSessionOptions.foreach { case (k, v) => state.conf.setConfString(k, v) }
        state
      }
  }
//é€šè¿‡åå­—åŒºåˆ†åˆ›å»ºé‚£ç§session
private def sessionStateClassName(conf: SparkConf): String = {
    conf.get(CATALOG_IMPLEMENTATION) match {
      case "hive" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME
      case "in-memory" => classOf[SessionStateBuilder].getCanonicalName
    }
  }

/**
   * Helper method to create an instance of `SessionState` based on `className` from conf.
   * The result is either `SessionState` or a Hive based `SessionState`.
   */
  private def instantiateSessionState(
      className: String,
	      sparkSession: SparkSession): SessionState = {
    try {
      // invoke `new [Hive]SessionStateBuilder(SparkSession, Option[SessionState])`
      val clazz = Utils.classForName(className) //é€šè¿‡åå°„åˆ›å»ºsession
      val ctor = clazz.getConstructors.head
// æ³¨æ„ï¼šè¿™é‡Œå°†hive | in-memoryçš„ classNameæ„å»ºæˆäº†ç»Ÿä¸€çš„çˆ¶ç±»BaseSessionStateBuilderï¼Œ
å¹¶ä¸”è°ƒç”¨äº†..build()å‡½æ•°
private def instantiateSessionState(
      ctor.newInstance(sparkSession, None).asInstanceOf[BaseSessionStateBuilder].build()
    } catch {
      case NonFatal(e) =>
        throw new IllegalArgumentException(s"Error while instantiating '$className':", e)
    }
  }
```

çœ‹ä¸€ä¸‹BaseSessionStateBuilderçš„ç»§æ‰¿å…³ç³»ï¼š

![Untitled](https://vscodepic.oss-cn-beijing.aliyuncs.com/blog/Untitled%204.png)

çœ‹ä¸€ä¸‹åˆ›å»ºsessionstateçš„build()æ–¹æ³•

```scala
/**
   * Build the [[SessionState]].
   */
  def build(): SessionState = {
    new SessionState(
      session.sharedState,
      conf,
      experimentalMethods,
      functionRegistry,
      udfRegistration,
      () => catalog,// catalogå…ƒæ•°æ®ï¼Œåé¢ä¼šåœ¨catalogä¸€èŠ‚å•ç‹¬ä»‹ç»
      sqlParser,// sqlè§£ææ ¸å¿ƒç±»
      () => analyzer,// analyzeré˜¶æ®µæ ¸å¿ƒç±»
      () => optimizer,// optimizeré˜¶æ®µæ ¸å¿ƒç±»
      planner,// ç‰©ç†è®¡åˆ’å’Œé”¡ç±»
      streamingQueryManager,
      listenerManager,
      () => resourceLoader,
      createQueryExecution,
      createClone)
  }

//åˆ›å»ºsqlè§£æå™¨
/**
   * Parser that extracts expressions, plans, table identifiers etc. from SQL texts.
   *ä»SQLæ–‡æœ¬ä¸­æå–è¡¨è¾¾å¼ã€è®¡åˆ’ã€è¡¨æ ‡è¯†ç¬¦ç­‰çš„è§£æå™¨ã€‚
   * Note: this depends on the `conf` field.
			ä¾èµ–äºé…ç½®æ–‡ä»¶conf
   */
  protected lazy val sqlParser: ParserInterface = {
    extensions.buildParser(session, new SparkSqlParser(conf))
  }

//åˆ›å»ºAnalyzerè§£æå™¨
/**
   * Logical query plan analyzer for resolving unresolved attributes and relations.
   *ç”¨äºè§£ææœªè§£æå±æ€§å’Œå…³ç³»çš„é€»è¾‘æŸ¥è¯¢è®¡åˆ’åˆ†æå™¨ã€‚
   * Note: this depends on the `conf` and `catalog` fields.
			æ³¨æ„ï¼šè¿™å–å†³äºâ€œconfâ€å’Œâ€œcatalogâ€å­—æ®µã€‚
   */
  protected def analyzer: Analyzer = new Analyzer(catalog, conf) {
    override val extendedResolutionRules: Seq[Rule[LogicalPlan]] =
      new FindDataSourceTable(session) +:
        new ResolveSQLOnFile(session) +:
        customResolutionRules

    override val postHocResolutionRules: Seq[Rule[LogicalPlan]] =
      PreprocessTableCreation(session) +:
        PreprocessTableInsertion(conf) +:
        DataSourceAnalysis(conf) +:
        customPostHocResolutionRules

    override val extendedCheckRules: Seq[LogicalPlan => Unit] =
      PreWriteCheck +:
        PreReadCheck +:
        HiveOnlyCheck +:
        customCheckRules
  }

// åˆ›å»ºoptimizerä¼˜åŒ–å™¨ç±»
/**
   * Logical query plan optimizer.
   *é€»è¾‘æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–å™¨ã€‚
   * Note: this depends on `catalog` and `experimentalMethods` fields.
		æ³¨æ„ï¼šè¿™å–å†³äºâ€œcatalogâ€å’Œâ€œå®éªŒæ–¹æ³•â€å­—æ®µã€‚
   */
  protected def optimizer: Optimizer = {
    new SparkOptimizer(catalog, experimentalMethods) {
      override def extendedOperatorOptimizationRules: Seq[Rule[LogicalPlan]] =
        super.extendedOperatorOptimizationRules ++ customOperatorOptimizationRules
    }
  }

// åˆ›å»ºplannerç‰©ç†è®¡åˆ’ç±»
/**
   * Planner that converts optimized logical plans to physical plans.
   *å°†ä¼˜åŒ–çš„é€»è¾‘è®¡åˆ’è½¬æ¢ä¸ºç‰©ç†è®¡åˆ’çš„è®¡åˆ’å™¨ã€‚
   * Note: this depends on the `conf` and `experimentalMethods` fields.
   */
  protected def planner: SparkPlanner = {
    new SparkPlanner(session.sparkContext, conf, experimentalMethods) {
      override def extraPlanningStrategies: Seq[Strategy] =
        super.extraPlanningStrategies ++ customPlanningStrategies
    }
  }
```

**è‡³æ­¤sqlå„ä¸ªé˜¶æ®µçš„æ ¸å¿ƒç±»åˆ›å»ºå‡†å¤‡å®Œæˆï¼Œæ¯ç§æ ¸å¿ƒç±»çš„ä½¿ç”¨ä¼šåœ¨åé¢å„ä¸ªé˜¶æ®µçš„æ–‡ç§ä¸­è¯¦ç»†å±•å¼€ï¼Œè¯·å…³æ³¨ä¸‹èŠ‚å†…å®¹ã€‚**

æœ€åæˆ‘ä»¬åœ¨å›é¡¾ä¸€ä¸‹ä¸Šé¢çš„æµç¨‹ï¼Œå‘ç°ä¸Šé¢çš„ 4 ä¸ªç±»/æ¥å£å¯¹åº”çš„å°±æ˜¯å‰é¢æåˆ°çš„ 4 å¤§é˜¶

![Untitled](https://vscodepic.oss-cn-beijing.aliyuncs.com/blog/Untitled%205.png)

å°†è¿™ 4 ä¸ªé˜¶æ®µä¸²è”èµ·æ¥ï¼Œå°±å¾—åˆ°äº† Spark SQL æœ€åŸºæœ¬çš„å·¥ä½œæµç¨‹ï¼š

![Untitled](https://vscodepic.oss-cn-beijing.aliyuncs.com/blog/Untitled%206.png)

æ³¨ï¼šæ­¤å¤„ç»™å‡ºçš„æ˜¯åŸºæœ¬çš„å·¥ä½œæµç¨‹ï¼Œå…¶ä¸­ä¸åŒ…å« AQEï¼ˆè‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œï¼‰, CBOï¼ˆåŸºäºæˆæœ¬çš„ä¼˜åŒ–ï¼‰ï¼ŒWSCGï¼ˆå…¨é˜¶æ®µä»£ç ç”Ÿæˆï¼‰ç­‰ï¼Œå®Œæ•´çš„æµç¨‹ä¼šåœ¨æœ€åä¸€è®²è®²å®Œä¸Šè¿°å‡ ä¸ªæ¦‚å¿µä¹‹åå†ç»™å‡ºã€‚